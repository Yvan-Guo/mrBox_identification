# -*- coding: utf-8 -*-
"""
Created on Mon Feb 27 22:37:42 2023

@author: Yvan Guo
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os
import time
import pandas as pd

# Set random seed for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

# Define hyperparameters
max_features = 5
maxlen = 512
category = 12 # depending on the result of MeshClust

# Input layer: variable-length integer sequences
inputs = keras.Input(shape=(maxlen,), dtype="int32")
# Embedding layer: maps each integer to a 128-dimensional vector
x = layers.Embedding(max_features, 128)(inputs)

# Convolutional Neural Network architecture with batch normalization for stability
x = layers.Conv1D(filters=256, kernel_size=8, padding="same", strides=2, activation="relu")(x)
x = layers.BatchNormalization()(x)  # Added batch normalization to improve training stability
x = layers.MaxPooling1D(1)(x)
x = layers.Conv1D(filters=128, kernel_size=8, padding="same", strides=2, activation="relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling1D(1)(x)
x = layers.Conv1D(filters=128, kernel_size=8, padding="same", strides=2, activation="relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling1D(1)(x)
x = layers.Conv1D(filters=32, kernel_size=8, padding="same", strides=2, activation="relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling1D(1)(x)
x = layers.Conv1D(filters=32, kernel_size=8, padding="same", strides=2, activation="relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling1D(1)(x)
x = layers.Conv1D(filters=16, kernel_size=8, padding="same", strides=2, activation="relu")(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling1D(1)(x)
x = layers.Dropout(rate=0.5)(x)  # Increased dropout rate to prevent overfitting
x = layers.Flatten()(x)

# Classifier output layer
outputs = layers.Dense(category, activation="softmax")(x)  # Using softmax for compatibility with categorical_crossentropy
encoder = keras.Model(inputs, outputs)

# Output model structure for review
encoder.summary()

# Define optimizer and learning rate schedule for better training dynamics
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=1000, decay_rate=0.9
)
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)

# Compile model, adapting to the standard answer test set
encoder.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=[
        keras.metrics.CategoricalAccuracy(name="accuracy"),
        #keras.metrics.Precision(name="precision"),
        #keras.metrics.Recall(name="recall")
    ]
)

# Data source explanation for clarity in publication
print("Model training")

'''
Training & testing set labels (y_train, y_test) are unsupervisely generated by MeshClust v3.0.
Entire dataset (x_all, y_all) consists of standard labels for evaluation.
'''
# Set environment and logs for training monitoring
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
log_dir = "./logs"
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Record training start time for running time comparison
Ts = time.time()

# Train model, adding learning rate schedule and longer EarlyStopping patience
history = encoder.fit(
    x_train, y_train, 
    epochs=1000,
    batch_size=500,
    validation_data=(x_test, y_test),
    callbacks=[
        tensorboard_callback,
        keras.callbacks.EarlyStopping(monitor="val_loss", patience=10, mode="min", restore_best_weights=True)  # Increased patience for better convergence
    ]
)

# Clustering sequence
clusters = encoder.predict(x_all)
pd.DataFrame(clusters).to_csv('clusters_all.csv', index=False)

Te = time.time()
print('Running time: %s s' % (Te - Ts))

# Evaluate model performance on test set with standard labels
test_metrics = encoder.evaluate(x_all, y_all, batch_size=32, verbose=1)
print("Test set evaluation - Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}".format(
    test_metrics[0], test_metrics[1], test_metrics[2], test_metrics[3]
))

# Save training history data for visualization and publication figures
history_df = pd.DataFrame(history.history)
history_df.to_csv('training_history.csv', index=False)
print("Training history saved to 'training_history.csv'.")

